---
# Kubernetes node configuration
# Do not change profile_name, configured_nic and configured_arch here !!!
# To generate vars for different profile/architecture use make command
# generated for profile and arch:
profile_name: build_your_own
configured_arch: skl
configured_nic: cvl

# Enable IOMMU (required for SR-IOV networking and QAT)
iommu_enabled: false

# dataplane interface configuration list
dataplane_interfaces: []
#  - bus_info: "18:00.0"                    # pci bus info
#    pf_driver: ice                         # PF driver, "i40e", "ice"
#    ddp_profile: "ice_comms-1.3.37.0.pkg"  # DDP package name to be loaded into the NIC
                                            # For i40e(XV710-*) allowable ddp values are: "ecpri.pkg", "esp-ah.pkg", "ppp-oe-ol2tpv2.pkgo", "mplsogreudp.pkg" and "gtp.pkgo", replace as required
                                            # For ice(E810-*) allowable ddp values are: ice_comms-1.3.[17,20,22,24,28,30,31,35].0.pkg  such as "ice_comms-1.3.37.0.pkg", replace as required
                                            # ddp_profile must be defined for first port of each network device. bifurcated cards will appear as unique devices.

#    flow_configuration: false              # Flow Configuration # NOTE: this option is for Intel E810 Series NICs and requires Intel Ethernet Operator and Flow Config to be enabled in group vars.
                                            # with Flow Configuration enabled the first VF (VF0) will be reserved for Flow Configuration and the rest of VFs will be indexed starting from 1.

#    default_vf_driver: "iavf"              # default driver to be used with VFs if specific driver is not defined in the "sriov_vfs" section
#    sriov_numvfs: 6                        # total number of VFs to create including VFs listed in the "sriov_vfs" section.
                                            # If total number of VFs listed in the "sriov_vfs" section is greater than "sriov_numvfs" then excessive entities will be ignored.
                                            # VF's name should follow scheme: <arbitrary_vf_name>_<zero_started_index_of_vf>
                                            # If index in the VF's name is greater than "sriov_numfs - 1" such VF will be ignored.
#    minio_vf: true

#    sriov_vfs:                             # list of VFs to create on this PF with specific driver
#      vf_00: "vfio-pci"                    # VF driver to be attached to this VF under this PF. Options: "iavf", "vfio-pci", "igb_uio"
#      vf_05: "vfio-pci"

#  - bus_info: "18:00.1"
#    pf_driver: ice
#    ddp_profile: "ice_comms-1.3.37.0.pkg"
#    default_vf_driver: "vfio-pci"
#    flow_configuration: false

#    sriov_numvfs: 4
#    minio_vf: true

#    sriov_vfs: {}                   # no VFs with specific driver on this PF or "sriov_vfs" can be omitted for convenience

# Set to 'true' to update i40e, ice and iavf kernel modules
update_nic_drivers: false
#i40e_driver_version: "2.20.12" # Downgrading i40e drivers is not recommended due to the possible consequences. Users should update and proceed at their own risk.
#i40e_driver_checksum: "sha1:a24f0c5512af31c68cd90667d5822121780d5487" # update checksum per required i40e drivers version
#ice_driver_version: "1.9.11"    # Downgrading ice drivers is not recommended due to the possible consequences. Users should update and proceed at their own risk.
#ice_driver_checksum: "sha1:f05e2322a66de5d4019e7aa6141a109bb419dda4"  # update checksum per required ice drivers version
#iavf_driver_version: "4.5.3" # Downgrading iavf drivers is not recommended due to the possible consequences. Users should update and proceed at their own risk.
#iavf_driver_checksum: "sha1:76b3a7dec392e559dea6112fa55f5614857cff2a" # update checksum per required iavf drivers version

# Set 'true' to upgrade / downgrade NIC firmware. FW upgrade / downgrade will be executed on all NICs listed in "dataplane_interfaces[*].bus_info".
update_nic_firmware: false # Note: downgrading FW is not recommended, users should proceed at their own risk.
#nvmupdate: []  # remove '[]' in case of downgrading FW such as 'nvmupdate:'
#  ice: []      # remove '[]' in case of downgrading FW to get required version of NVM 'ICE' 800 Series such as 'ice:'
#     nvmupdate_pkg_url: "https://downloadmirror.intel.com/738715/E810_NVMUpdatePackage_v4_00_Linux.tar.gz"
#     nvmupdate_pkg_checksum: "sha1:7C168880082653B579FDF225A2E6E9301C154DD1"
#     required_fw_version: "4.0"
#      # https://builders.intel.com/docs/networkbuilders/intel-ethernet-controller-800-series-device-personalization-ddp-for-telecommunications-workloads-technology-guide.pdf
#      # document above does not specify any min fw version needed for ddp feature. So, min_ddp_loadable_fw is the same as min_updatable_fw
#     min_ddp_loadable_fw_version: "0.70"
#     min_updatable_fw_version: "0.70"
       # when downgrading only, the recommended below version is required to download the supported NVMupdate64E tool. Users should replace the tool at their own risk.
#     supported_nvmupdate_tool_pkg_url: "https://downloadmirror.intel.com/738715/E810_NVMUpdatePackage_v4_00_Linux.tar.gz"
#     supported_nvmupdate_tool_pkg_checksum: "sha1:7C168880082653B579FDF225A2E6E9301C154DD1"
#     supported_nvmupdate_tool_fw_version: "4.0"

# install Intel x700 & x800 series NICs DDP packages
install_ddp_packages: false
# If following error appears: "Flashing failed: Operation not permitted"
# run deployment with update_nic_firmware: true
# or
# Disable ddp installation via install_ddp_packages: false

# set 'true' to enable custom ddp package to be loaded after reboot
enable_ice_systemd_service: false

sriov_cni_enabled: false

# Custom SriovNetworkNodePolicy manifests local path
# custom_sriov_network_policies_dir: /tmp/sriov
# Bond CNI
bond_cni_enabled: false

# Install DPDK (required for SR-IOV networking)
install_dpdk: false
# DPDK version (will be in action if install_dpdk: true)
dpdk_version: "22.07"
# Custom DPDK patches local path
#dpdk_local_patches_dir: "/tmp/patches/dpdk"
# It might be necessary to adjust the patch strip parameter, update as required.
#dpdk_local_patches_strip: 0

# Userspace networking
userspace_cni_enabled: false
ovs_dpdk_enabled: false # Should be enabled with Userspace CNI, when VPP is set to "false"; 1G hugepages required
ovs_version: "v2.17.2"
# CPU mask for OVS-DPDK PMD threads
ovs_dpdk_lcore_mask: 0x1
# Huge memory pages allocated by OVS-DPDK per NUMA node in megabytes
# example 1: "256,512" will allocate 256MB from node 0 and 512MB from node 1
# example 2: "1024" will allocate 1GB from node 0 on a single socket board, e.g. in a VM
ovs_dpdk_socket_mem: "256,0"
vpp_enabled: false # Should be enabled with Userspace CNI, when ovs_dpdk is set to "false"; 2M hugepages required

# Enables hugepages support
hugepages_enabled: false
# Hugepage sizes available: 2M, 1G
default_hugepage_size: 1G
# Sets how many hugepages should be created
number_of_hugepages_1G: 4
number_of_hugepages_2M: 1024

# Intel Ethernet Operator for Intel E810 Series network interface cards
intel_ethernet_operator:
  ddp_update: false                   # perform DDP update on PFs listed in dataplane_interfaces using selected DDP profile
  fw_update: false                     # perform firmware update on PFs listed in dataplane_interfaces
  # NodeFlowConfig manifests local path
  # For more information refer to:
  # https://github.com/intel/intel-ethernet-operator/blob/main/docs/flowconfig-daemon/creating-rules.md
  # node_flow_config_dir: /tmp/node_flow_config

# Wireless FEC H/W Accelerator Device (e.g. ACC100) PCI ID
fec_acc: "0000:27:00.0"  # must be string in [a-fA-F0-9]{4}:[a-fA-F0-9]{2}:[01][a-fA-F0-9].[0-7] format

# Intel FlexRAN
intel_flexran_enabled: false # if true, deploy FlexRAN

# Enabling this feature will install QAT drivers + services
update_qat_drivers: false

# qat interface configuration list
qat_devices: []
#  - qat_id: "0000:ab:00.0"           # QAT device id one using DPDK compatible driver for VF devices to be used by vfio-pci kernel driver, replace as required
#    qat_sriov_numvfs: 12             # Number of VFs per PF to create - cannot exceed the maximum number of VFs available for the device. Set to 0 to not create any VFs.
                                      # Note: Currently when trying to create fewer virtual functions than the maximum, the maximum number always gets created
#  - qat_id: "0000:xy:00.0"
#    qat_sriov_numvfs: 10

#  - qat_id: "0000:yz:00.0"
#    qat_sriov_numvfs: 10

# Install and configure OpenSSL cryptography
openssl_install: false # This requires update_qat_drivers set to 'true' in host vars

# CPU isolation from Linux scheduler
isolcpus_enabled: false
isolcpus: "4-11"

# CPU shielding
cpusets_enabled: false
cpusets: "4-11"

# Native CPU Manager (Kubernetes built-in)
# These settings are relevant only if in group_vars native_cpu_manager_enabled: true
# Amount of CPU cores that will be reserved for the housekeeping (2000m = 2000 millicores = 2 cores)
native_cpu_manager_system_reserved_cpus: 2000m
# Amount of CPU cores that will be reserved for Kubelet
native_cpu_manager_kube_reserved_cpus: 1000m
# Explicit list of the CPUs reserved for the host level system threads and Kubernetes related threads
#native_cpu_manager_reserved_cpus: "0,1,2"
# Note: All remaining unreserved CPU cores will be consumed by the workloads.

cstate_enabled: false
cstates:
  C1: # default values: C6 for access, C1 for other profiles
    cpu_range: '0-9' # change as needed, cpus to modify cstates on
    enable: true # true - enable given cstate, false - disable given cstate

ufs_enabled: false
ufs: # uncore frequency scaling
  min: 1000 # minimal uncore frequency
  max: 2000 # maximal uncore frequency

# Intel Speed Select Base-Frequency configuration.

# Intel custom GPU kernel - this is required to be true in order to
# deploy Intel GPU Device Plugin on that node
configure_gpu: false

# Telemetry configuration
# intel_pmu plugin collects information provided by Linux perf interface.
enable_intel_pmu_plugin: false

# CPU Threads to be monitored by Intel PMU Plugin.
# If the field is empty, all available cores will be monitored.
# Please refer to https://collectd.org/wiki/index.php/Plugin:Intel_PMU for configuration details.
intel_pmu_plugin_monitored_cores: ""

# CPU Threads to be monitored by Intel RDT Plugin.
# If the field is empty, all available cores will be monitored.
# Please refer to https://collectd.org/wiki/index.php/Plugin:IntelRDT for configuration details.
intel_rdt_plugin_monitored_cores: ""

# Additional list of plugins that will be excluded from collectd deployment.
exclude_collectd_plugins: []

# Intel Cloud Native Data Plane.
cndp_enabled: false
cndp_dp_pools:
  - name: "e2e"
    drivers: "{{ dataplane_interfaces | map(attribute='pf_driver') | list | unique }}"   # List of NIC driver to be included in CNDP device plugin ConfigMap.


# MinIO storage configuration
minio_pv: []
#  - name: "mnt-data-1"                         # PV identifier will be used for PVs names followed by node name(e.g., mnt-data-1-hostname)
#    storageClassName: "local-storage"          # Storage class name to match with PVC
#    accessMode: "ReadWriteOnce"                # Access mode when mounting a volume, e.g., ReadWriteOnce/ReadOnlyMany/ReadWriteMany/ReadWriteOncePod
#    persistentVolumeReclaimPolicy: "Retain"    # Reclaim policy when a volume is released once it's bound, e.g., Retain/Recycle/Delete
#    capacity: 1GiB                             # Size of the PV. support only GiB/TiB
#    mountPath: /mnt/data0                      # Mount path of a volume
#    device: /dev/nvme0n1                       # Target storage device name when creating a volume.
                                                # When group_vars: minio_deploy_test_mode == true, use a file as a loop device for storage
                                                # otherwise, an actual NVME or SSD device for storage on the device name.

#  - name: "mnt-data-2"
#    storageClassName: "local-storage"
#    accessMode: "ReadWriteOnce"
#    persistentVolumeReclaimPolicy: "Retain"
#    capacity: 1GiB
#    mountPath: /mnt/data1
#    device: /dev/nvme1n1

#  - name: "mnt-data-3"
#    storageClassName: "local-storage"
#    accessMode: "ReadWriteOnce"
#    persistentVolumeReclaimPolicy: "Retain"
#    capacity: 1GiB
#    mountPath: /mnt/data2
#    device: /dev/nvme2n1

#  - name: "mnt-data-4"
#    storageClassName: "local-storage"
#    accessMode: "ReadWriteOnce"
#    persistentVolumeReclaimPolicy: "Retain"
#    capacity: 1GiB
#    mountPath: /mnt/data3
#    device: /dev/nvme3n1

#  - name: "mnt-data-5"
#    storageClassName: "local-storage"
#    accessMode: "ReadWriteOnce"
#    persistentVolumeReclaimPolicy: "Retain"
#    capacity: 1GiB
#    mountPath: /mnt/data4
#    device: /dev/nvme4n1

#  - name: "mnt-data-6"
#    storageClassName: "local-storage"
#    accessMode: "ReadWriteOnce"
#    persistentVolumeReclaimPolicy: "Retain"
#    capacity: 1GiB
#    mountPath: /mnt/data5
#    device: /dev/nvme5n1