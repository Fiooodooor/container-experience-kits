#!/usr/bin/env bash

KEYPAIR_PATH="fileb://{{ cloud_config.ssh_pub_key_path }}"
KEYPAIR_NAME="cwdf-infra-{{ cloud_config.job_id }}-keypair"

# Support files
SCRIPT_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
DATA_FILE="${SCRIPT_DIR}/aws_running_ids"
OUTPUT_FILE="${SCRIPT_DIR}/provision_output.json"

# Networking name variables
VPC_NAME="cwdf-infra-{{ cloud_config.job_id }}-default-vpc"
VPC_IG_NAME="cwdf-infra-{{ cloud_config.job_id }}-default-ig"
VPC_ROUTE_TABLE_NAME="cwdf-infra-{{ cloud_config.job_id }}-route-table"
SECURITY_GROUP_NAME="cwdf-infra-{{ cloud_config.job_id }}-security_group"
SECURITY_GROUP_DESCRIPTION="cwdf-infra-{{ cloud_config.job_id }}-security_group-cloudCLI"

# Amazon Elastic Container Registry name variable
ECR_REPOSITORY_NAME="cwdf-infra-{{ cloud_config.job_id }}-ecr-repository"

# Amazon Elastic Kubernetes Service variables (cluster roles)
IAM_EKS_CLUSTER_ROLE=$(echo -n '{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Principal":{"Service":"eks.amazonaws.com"},"Action":"sts:AssumeRole"}]}')
IAM_EKS_CLUSTER_NODERGROUP_ROLE=$(echo -n '{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Principal":{"Service":"ec2.amazonaws.com"},"Action":"sts:AssumeRole"}]}')

# Amazon Elastic Kubernetes Service role name variables
IAM_CLUSTER_ROLE_NAME="cwdf-infra-{{ cloud_config.job_id }}-eks-cluster-role"
IAM_NODEGROUP_ROLE_NAME="cwdf-infra-{{ cloud_config.job_id }}-eks-cluster-nodegroup-role"

# Amazon Elastic Kubernetes Service variables (policies)
IAM_EKS_POLICY_ARN="arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
IAM_EKS_WORKERNODE_POLICY_ARD="arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
IAM_EKS_CONTAINER_REGISTRY_READONLY_POLICY_ARD="arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
IAM_EKS_CNI_POLICY="arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"

# Amazon Elastic Kubernetes Service name variables
EKS_CLUSTER_NAME="cwdf-infra-{{ cloud_config.job_id }}-eks-cluster"
EKS_LAUNCH_TEMPLATE_NAME="cwdf-infra-{{ cloud_config.job_id }}-nodegroup-launch-template"
EKS_NODE_GROUP_NAME="cwdf-infra-{{ cloud_config.job_id }}-node-group"

# Amazon Elastic Kubernetes Service AMI ID
EKS_AMI_ID=$(aws ec2 describe-images --filters Name=name,Values=ubuntu-eks/k8s_{{ cloud_config.eks.kubernetes_version }}/images/* --query 'Images[?CreationDate>`2022-09-01`] | reverse(sort_by(@, &CreationDate))[0].ImageId' --output text)

# Ansible instance variables
ANSIBLE_INSTANCE_IMAGE=$(aws ec2 describe-images \
                                --filters Name=name,Values=ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-* \
                                --query 'Images[?CreationDate>`2022-09-01`] | reverse(sort_by(@, &CreationDate))[0].ImageId' --output text)
ANSIBLE_INSTANCE_TYPE="t3.medium"
ANSIBLE_INSTANCE_NAME="cwdf-infra-{{ cloud_config.job_id }}-ansible-instance"

# Generate Ansible instance SSH Host Key
if [ ! -f ./ansible_host ]
then
  ssh-keygen -q -N "" -t rsa -f ./ansible_host
fi
ANSIBLE_INSTANCE_HOST_PRIVKEY=$(cat ./ansible_host)
ANSIBLE_INSTANCE_HOST_PUBKEY=$(cat ./ansible_host.pub)

# Ansible instance entrypoint script
ANSIBLE_INSTANCE_ENTRYPOINT="$(cat <<- "EOM"
#!/usr/bin/env bash
echo $ANSIBLE_INSTANCE_HOST_PRIVKEY > /etc/ssh/ssh_host_rsa_key
rm /etc/ssh/ssh_host_dsa_key
rm /etc/ssh/ssh_host_ed25519_key
rm /etc/ssh/ssh_host_ecdsa_key
apt-get -qq -y update
apt-get -qq -y upgrade
apt-get -qq -y install python3-pip python3-venv
apt-get -qq -y install zip unzip net-tools apache2-utils
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "/tmp/awscliv2.zip"
unzip /tmp/awscliv2.zip -d /tmp/
/tmp/aws/install
rm -rf /tmp/aws
rm -rf /tmp/awscliv2.zip
curl -o /usr/local/bin/kubectl https://s3.us-west-2.amazonaws.com/amazon-eks/1.22.6/2022-03-09/bin/linux/amd64/kubectl
chmod +x /usr/local/bin/kubectl
sudo -H -u ubuntu bash -c 'aws eks --region {{ cloud_config.region }} update-kubeconfig --name cwdf-infra-{{ cloud_config.job_id }}-eks-cluster'
sudo cp -r /home/ubuntu/.kube /root/
curl -fsSL -o /tmp/get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 /tmp/get_helm.sh
/bin/bash /tmp/get_helm.sh
rm /tmp/get_helm.sh
mkdir /home/ubuntu/cwdf_deployment
mkdir /home/ubuntu/cwdf_deployment/ssh
echo "{{ cloud_config.ssh_public_key }}" >> /home/ubuntu/cwdf_deployment/ssh/id_rsa.pub
chown ubuntu /home/ubuntu/cwdf_deployment -R
sudo -H -u ubuntu bash -c 'pip install --user paramiko scp'
EOM
)"

# Check if file with running IDs exists
if test -f "$DATA_FILE"; then
    printf 'File aws_running_ids was detected.'
    printf 'This may mean that there are running instances in AWS.'
    printf 'If you choose to continue, the script will try to delete the previous instances.'
    printf 'Do you want to continue (y/n)? '
    old_stty_cfg=$(stty -g)
    stty raw -echo ; answer=$(head -c 1) ; stty $old_stty_cfg
    if echo "$answer" | grep -iq "^y" ;then
        ./aws_cloudcli_cleanup.sh
    else
        exit 0
    fi
fi

# Array and variable for saving subnets IDs
declare -A subnets
SUBNETS_IDS=""

# Save IDs of created instances and network stuff
touch $DATA_FILE

#Show JOB ID
echo "Running AWS CloudCLI job with ID: {{ cloud_config.job_id }}"

#Creating VPC with subnet
echo "--------------------------------------------------"
echo "Creating VPC"
VPC_ID=$(aws ec2 create-vpc \
  --cidr-block {{ cloud_config.vpc_cidr_block }} \
  --tag-specifications 'ResourceType=vpc,Tags=[{Key=Name,Value='${VPC_NAME}'}, {% for tag in cloud_config.extra_tags %}{Key={{ tag }},Value={{ cloud_config.extra_tags[tag] }}}{{ ", " if not loop.last else "" }}{% endfor %}]' | jq '.Vpc.VpcId' | tr -d '"')
echo "VPC_ID=$(echo $VPC_ID)" >> $DATA_FILE

#Creating subnet in VPC
{%- for subnet in cloud_config.subnets %}
SUBNET_NAME="cwdf-infra-{{ cloud_config.job_id }}-{{ subnet.name }}"
echo "--------------------------------------------------"
echo "Creating subnet: ${SUBNET_NAME}"
SUBNET_ID=$(aws ec2 create-subnet \
  --vpc-id $VPC_ID \
  --cidr-block {{ subnet.cidr_block }} \
  --availability-zone {{ subnet.az }} \
  --tag-specifications 'ResourceType=subnet,Tags=[{Key=Name,Value='"${SUBNET_NAME}"'}, {% for tag in cloud_config.extra_tags %}{Key={{ tag }},Value={{ cloud_config.extra_tags[tag] }}}{{ ", " if not loop.last else "" }}{% endfor %}]' | jq '.Subnet.SubnetId' | tr -d '"')
subnets['{{ subnet.name }}']=$(echo $SUBNET_ID)
if [ -z "$SUBNETS_IDS" ]
then
      SUBNETS_IDS="$SUBNET_ID"
else
      SUBNETS_IDS="$SUBNETS_IDS,$SUBNET_ID"
fi
echo $SUBNET_ID
{% endfor %}

# Creating IG for access the Internet from EC2 instance
echo "--------------------------------------------------"
echo "Creating internet gateway: ${VPC_IG_NAME}"
VPC_IGW=$(aws ec2 create-internet-gateway \
  --tag-specifications 'ResourceType=internet-gateway,Tags=[{Key=Name,Value='${VPC_IG_NAME}'}, {% for tag in cloud_config.extra_tags %}{Key={{ tag }},Value={{ cloud_config.extra_tags[tag] }}}{{ ", " if not loop.last else "" }}{% endfor %}]' | jq '.InternetGateway.InternetGatewayId' | tr -d '"')
echo "VPC_IGW=$(echo $VPC_IGW)" >> $DATA_FILE

# Add IG to the VPC
echo "--------------------------------------------------"
echo "Adding internet gateway to VPC"
aws ec2 attach-internet-gateway \
  --vpc-id $VPC_ID \
  --internet-gateway-id $VPC_IGW

# Creating routing table for VPC
echo "--------------------------------------------------"
echo "Creating routing table for VPC"
ROUTE_TABLE_ID=$(aws ec2 create-route-table \
  --vpc-id $VPC_ID \
  --tag-specifications 'ResourceType=route-table,Tags=[{Key=Name,Value='${VPC_ROUTE_TABLE_NAME}'}, {% for tag in cloud_config.extra_tags %}{Key={{ tag }},Value={{ cloud_config.extra_tags[tag] }}}{{ ", " if not loop.last else "" }}{% endfor %}]' | jq '.RouteTable.RouteTableId' | tr -d '"')
echo "ROUTE_TABLE_ID=$(echo $ROUTE_TABLE_ID)" >> $DATA_FILE

#Creating route in routing table (Internet access)
echo "--------------------------------------------------"
echo "Creating route to internet"
aws ec2 create-route \
  --route-table-id $ROUTE_TABLE_ID \
  --destination-cidr-block 0.0.0.0/0 \
  --gateway-id $VPC_IGW \
  --output text

# Add routing table to VPC
{%- for subnet in cloud_config.subnets %}
aws ec2 associate-route-table \
  --subnet-id=${subnets['{{ subnet.name }}']} \
  --route-table-id $ROUTE_TABLE_ID \
  --output text

#Get public IP from start and add it to created VPC
aws ec2 modify-subnet-attribute \
  --subnet-id=${subnets['{{ subnet.name }}']} \
  --map-public-ip-on-launch \
  --output text
{% endfor %}

if [ -z "$KEYPAIR_PATH" ]
then
# Creating key-pair for accessing EC2 instance via SSH
echo "--------------------------------------------------"
echo "Creating SSH key-pair"
aws ec2 create-key-pair \
  --key-name $KEYPAIR_NAME \
  --query "KeyMaterial" --output text > $KEYPAIR_PATH
else
echo "--------------------------------------------------"
echo "Importing SSH public key"
KEYPAIR_ID=$(aws ec2 import-key-pair \
  --key-name $KEYPAIR_NAME \
  --public-key-material $KEYPAIR_PATH \
  --output json | jq '.KeyPairId' | tr -d '"')
fi
echo "KEYPAIR_NAME=$(echo $KEYPAIR_NAME)" >> $DATA_FILE
echo "KEYPAIR_ID=$(echo $KEYPAIR_ID)" >> $DATA_FILE

# Creating security group
echo "--------------------------------------------------"
echo "Creating security group"
SECURITY_GROUP_ID=$(aws ec2 create-security-group \
  --group-name $SECURITY_GROUP_NAME \
  --description $SECURITY_GROUP_DESCRIPTION \
  --vpc-id $VPC_ID \
  --tag-specifications 'ResourceType=security-group,Tags=[{% for tag in cloud_config.extra_tags %}{Key={{ tag }},Value={{ cloud_config.extra_tags[tag] }}}{{ ", " if not loop.last else "" }}{% endfor %}]' | jq '.GroupId' | tr -d '"')
echo "SECURITY_GROUP_ID=$(echo $SECURITY_GROUP_ID)" >> $DATA_FILE

# Authorize access on port 22
echo "--------------------------------------------------"
echo "Enable SSH access"
{% for address in cloud_config.sg_whitelist_cidr_blocks -%}
aws ec2 authorize-security-group-ingress \
  --region {{ cloud_config.region }} \
  --group-id $SECURITY_GROUP_ID \
  --protocol tcp \
  --port 22 \
  --cidr {{ address }} \
  --output text
{% endfor %}

# Creating EKS roles
echo "--------------------------------------------------"
echo "Creating EKS roles"
IAM_CLUSTER_ROLE_ARN=$(aws iam create-role \
  --role-name $IAM_CLUSTER_ROLE_NAME \
  --description "Kubernetes administrator role (for AWS IAM Authenticator for Kubernetes)." \
  --assume-role-policy-document $IAM_EKS_CLUSTER_ROLE \
  --output json \
  --tags '[{% for tag in cloud_config.extra_tags %}{"Key":"{{ tag }}","Value":"{{ cloud_config.extra_tags[tag] }}"}{{ ", " if not loop.last else "" }}{% endfor %}]' | jq '.Role.Arn' | tr -d '"')
echo "IAM_CLUSTER_ROLE_ARN=$(echo $IAM_CLUSTER_ROLE_ARN)" >> $DATA_FILE
echo "IAM_CLUSTER_ROLE_NAME=$(echo $IAM_CLUSTER_ROLE_NAME)" >> $DATA_FILE

IAM_NODEGROUP_ROLE_ARN=$(aws iam create-role \
  --role-name $IAM_NODEGROUP_ROLE_NAME \
  --description "Kubernetes administrator role (for AWS IAM Authenticator for Kubernetes)." \
  --assume-role-policy-document $IAM_EKS_CLUSTER_NODERGROUP_ROLE \
  --output json \
  --tags '[{% for tag in cloud_config.extra_tags %}{"Key":"{{ tag }}","Value":"{{ cloud_config.extra_tags[tag] }}"}{{ ", " if not loop.last else "" }}{% endfor %}]' | jq '.Role.Arn' | tr -d '"')
echo "IAM_NODEGROUP_ROLE_NAME=$(echo $IAM_NODEGROUP_ROLE_NAME)" >> $DATA_FILE

# Attaching roles to EKS policy
echo "--------------------------------------------------"
echo "Attaching roles to EKS policy"
aws iam attach-role-policy --role-name $IAM_CLUSTER_ROLE_NAME --policy-arn $IAM_EKS_POLICY_ARN
aws iam attach-role-policy --role-name $IAM_NODEGROUP_ROLE_NAME --policy-arn $IAM_EKS_POLICY_ARN
aws iam attach-role-policy --role-name $IAM_NODEGROUP_ROLE_NAME --policy-arn $IAM_EKS_WORKERNODE_POLICY_ARD
aws iam attach-role-policy --role-name $IAM_NODEGROUP_ROLE_NAME --policy-arn $IAM_EKS_CONTAINER_REGISTRY_READONLY_POLICY_ARD
aws iam attach-role-policy --role-name $IAM_NODEGROUP_ROLE_NAME --policy-arn $IAM_EKS_CNI_POLICY
echo "IAM_EKS_POLICY_ARN=$(echo $IAM_EKS_POLICY_ARN)" >> $DATA_FILE
echo "IAM_EKS_WORKERNODE_POLICY_ARD=$(echo $IAM_EKS_WORKERNODE_POLICY_ARD)" >> $DATA_FILE
echo "IAM_EKS_CONTAINER_REGISTRY_READONLY_POLICY_ARD=$(echo $IAM_EKS_CONTAINER_REGISTRY_READONLY_POLICY_ARD)" >> $DATA_FILE
echo "IAM_EKS_CNI_POLICY=$(echo $IAM_EKS_CNI_POLICY)" >> $DATA_FILE

# Creating EKS
echo "--------------------------------------------------"
echo "Creating EKS"
aws eks create-cluster \
  --region {{ cloud_config.region }} \
  --name $EKS_CLUSTER_NAME \
  --kubernetes-version {{ cloud_config.eks.kubernetes_version }} \
  --role-arn $IAM_CLUSTER_ROLE_ARN \
  --resources-vpc-config subnetIds=$SUBNETS_IDS,securityGroupIds=$SECURITY_GROUP_ID \
  --output text \
  --tags '{% for tag in cloud_config.extra_tags %}{{ "{" if loop.first }}"{{ tag }}":"{{ cloud_config.extra_tags[tag] }}"{{ ", " if not loop.last else "}" }}{% endfor %}'

while [ "$(aws eks describe-cluster --name $EKS_CLUSTER_NAME | jq '.cluster.status' | tr -d '"')" != "ACTIVE" ]
do
  echo "Waiting for EKS cluster to be created."
  echo "Next try in 30s."
  sleep 30s
done
echo "EKS_CLUSTER_NAME=$(echo $EKS_CLUSTER_NAME)" >> $DATA_FILE

# Get CA of EKS cluster
EKS_CLUSTER_CA=$(aws eks describe-cluster --name $EKS_CLUSTER_NAME | jq '.cluster.certificateAuthority.data' | tr -d '"' | tr -d '\n')
#Get API K8s endpoint of EKS cluster
EKS_CLUSTER_API_ENDPOINT_URL=$(aws eks describe-cluster --name $EKS_CLUSTER_NAME | jq '.cluster.endpoint' | tr -d '"')
EKS_SECURITY_GROUP_ID=$(aws eks describe-cluster --name $EKS_CLUSTER_NAME | jq '.cluster.resourcesVpcConfig.clusterSecurityGroupId' | tr -d '"')

EKS_NODEGROUP_INSTANCE_ENTRYPOINT=$(cat <<- EOM
MIME-Version: 1.0
Content-Type: multipart/mixed; boundary="==BOUNDARY=="

--==BOUNDARY==
Content-Type: text/x-shellscript; charset="us-ascii"

#!/bin/bash
/etc/eks/bootstrap.sh $EKS_CLUSTER_NAME \
  --b64-cluster-ca $EKS_CLUSTER_CA \
  --apiserver-endpoint $EKS_CLUSTER_API_ENDPOINT_URL \
  --dns-cluster-ip 172.20.0.10
echo "{{ cloud_config.ssh_public_key }}" >> /home/ubuntu/.ssh/id_rsa.pub

--==BOUNDARY==--
EOM
)

EKS_NODEGROUP_INSTANCE_ENTRYPOINT=$(echo "$EKS_NODEGROUP_INSTANCE_ENTRYPOINT" | base64 -w 0)

echo "--------------------------------------------------"
echo "Creating launch template for nodegroup"
EKS_NODEGROUP_LAUNCH_TEMPLATE=$(aws ec2 create-launch-template \
                                  --launch-template-name $EKS_LAUNCH_TEMPLATE_NAME \
                                  --launch-template-data "{\"KeyName\":\"${KEYPAIR_NAME}\", \"SecurityGroupIds\":[\"${SECURITY_GROUP_ID}\", \"${EKS_SECURITY_GROUP_ID}\"], \"ImageId\":\"${EKS_AMI_ID}\", \"UserData\":\"${EKS_NODEGROUP_INSTANCE_ENTRYPOINT}\"}" | jq '.LaunchTemplateId' | tr -d '"')
echo "EKS_LAUNCH_TEMPLATE_NAME=$(echo $EKS_LAUNCH_TEMPLATE_NAME)" >> $DATA_FILE

# Replace the comma with a space because the command to create a node group has a different argument syntax
SUBNETS_IDS=${SUBNETS_IDS/,/ }

# Creating EKS node group
{%- for node_group in cloud_config.eks.node_groups %}
echo "--------------------------------------------------"
echo "Creating nodegroup - {{ node_group.name }}"
aws eks create-nodegroup --cluster-name $EKS_CLUSTER_NAME \
                         --subnets $SUBNETS_IDS \
                         --region {{ cloud_config.region }} \
                         --nodegroup-name {{ node_group.name }} \
                         --instance-type {{ node_group.instance_type }} \
                         --node-role $IAM_NODEGROUP_ROLE_ARN \
                         --output text \
                         --scaling-config minSize={{ node_group.vm_count }},maxSize={{ node_group.vm_count }},desiredSize={{ node_group.vm_count }} \
                         {% if cloud_config.eks.ubuntu_ami_id is defined %}--launch-template name=$EKS_LAUNCH_TEMPLATE_NAME \{% endif %}
                         --tags '{% for tag in cloud_config.extra_tags %}{{ "{" if loop.first }}"{{ tag }}":"{{ cloud_config.extra_tags[tag] }}"{{ ", " if not loop.last else "}" }}{% endfor %}'

while [ "$(aws eks describe-nodegroup --cluster-name $EKS_CLUSTER_NAME --nodegroup-name {{ node_group.name }} | jq '.nodegroup.status' | tr -d '"')" != "ACTIVE" ]
do
  echo "Waiting for EKS node group to be created."
  echo "Next try in 30s."
  sleep 30s
done
{% endfor %}

ECR_URL=$(aws ecr create-repository --repository-name $ECR_REPOSITORY_NAME \
                          --tags '[{% for tag in cloud_config.extra_tags %}{"Key":"{{ tag }}","Value":"{{ cloud_config.extra_tags[tag] }}"}{{ ", " if not loop.last else "" }}{% endfor %}]' | jq '.repository.repositoryUri' | tr -d '"')
echo "ECR_REPOSITORY_NAME=$(echo $ECR_REPOSITORY_NAME)" >> $DATA_FILE

# Creating AWS EC2 instance
echo "--------------------------------------------------"
echo "Creating ansible instance"
{%- if cloud_config.will_create_ansible_instance == True %}
INSTANCE_ID=$(aws ec2 run-instances \
  --image-id $ANSIBLE_INSTANCE_IMAGE \
  --count 1 \
  --instance-type $ANSIBLE_INSTANCE_TYPE \
  --key-name $KEYPAIR_NAME \
  --security-group-ids $SECURITY_GROUP_ID \
  --subnet-id ${subnets['{{ cloud_config.subnets[0].name }}']} \
  --user-data "${ANSIBLE_INSTANCE_ENTRYPOINT}" \
  --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value="cwdf-infra-{{ cloud_config.job_id }}-ansible-instance"},{% for tag in cloud_config.extra_tags %}{Key={{ tag }},Value={{ cloud_config.extra_tags[tag] }}}{{ ", " if not loop.last else "" }}{% endfor %}]' | jq '.Instances[0].InstanceId' | tr -d '"')
INSTANCE_INFO=$(aws ec2 describe-instances --instance-ids $INSTANCE_ID)
ANSIBLE_INSTANCE_PUBLIC_IP=$(echo $INSTANCE_INFO | jq '.Reservations[0].Instances[0].PublicIpAddress' | tr -d '"')
echo "INSTANCE_ID=$(echo $INSTANCE_ID)" >> $DATA_FILE
echo "ANSIBLE_INSTANCE_NAME=$(echo $ANSIBLE_INSTANCE_NAME)" >> $DATA_FILE
echo "--------------------------------------------------"
echo "Wait until an instance is running"
aws ec2 wait instance-running --instance-ids $INSTANCE_ID
{% endif %}

# Create output JSON
JSON_OUTPUT=$(jq -n \
                 --arg ansible_host_ip "$ANSIBLE_INSTANCE_PUBLIC_IP" \
                 --arg cloud_provider "aws" \
                 --arg cr_url $ECR_URL \
                 --arg k8s_worker_username "ubuntu" \
                 --arg host_key "$ANSIBLE_INSTANCE_HOST_PUBKEY" \
                 '{ansible_host_public_ip:
                    {value: $ansible_host_ip},
                   cloud_provider: 
                    {value: $cloud_provider},
                   ansible_ssh_host_key:
                    {value: $host_key},
                   cr_url: 
                    {value: $cr_url},
                   k8s_worker_username: 
                    {value: $k8s_worker_username},
                   k8s_worker_instances: 
                    {value: []}}')

for node_group in $(aws eks list-nodegroups --cluster-name $EKS_CLUSTER_NAME | jq .nodegroups[])
do
        asg=$(aws eks describe-nodegroup --cluster-name $EKS_CLUSTER_NAME --nodegroup-name default | jq .nodegroup.resources.autoScalingGroups[0].name | tr -d '"')
        for instance in $(aws autoscaling describe-auto-scaling-groups --auto-scaling-group-names $asg | jq .AutoScalingGroups[0].Instances[].InstanceId | tr -d '"')
        do
                PRIVATE_IP=$(aws ec2 describe-instances --instance-ids "$instance" | jq ".Reservations[].Instances[].PrivateIpAddress" | tr -d '"')
                PUBLIC_IP=$(aws ec2 describe-instances --instance-ids "$instance" | jq ".Reservations[].Instances[].PublicIpAddress" | tr -d '"')
                echo "Worker $instance"
                echo "Private IP $PRIVATE_IP"
                echo "Public IP $PUBLIC_IP"
                JSON_OUTPUT=$( echo $JSON_OUTPUT | jq \
                --arg w_id "$instance" \
                --arg pr_ip "$PRIVATE_IP" \
                --arg pub_ip "$PUBLIC_IP" \
                '.k8s_worker_instances.value += [{
                id: $w_id,
                private_ip: $pr_ip,
                public_ip: $pub_ip}]')
        done
done

# Create output file
touch $OUTPUT_FILE
echo $JSON_OUTPUT >> $OUTPUT_FILE
