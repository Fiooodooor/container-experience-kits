---
###########################
## Profile Configuration ##
###########################
## Do not modify values listed here
## Re-run the "make" command to change profile configuration

profile_name: {{ name }}
configured_arch: {{ arch }}

{% if vm_mode in ['optional'] %}
# vm_enabled can't be enabled manually here
# To enable it, vm specific configuration from examples/vm need to be taken
{% endif %}
vm_enabled: {% if vm_mode == 'on' %}true{% else %}false{% endif %}


{% if cloud_mode == 'on' %}
# on_cloud is used when deploying Cloud RA
on_cloud: true

{% endif %}
#########################
## Misc. Configuration ##
#########################

# Preflight will check vars configuration
# It is NOT recommended to disable preflight, unless it is a conscious decision
preflight_enabled: true

unconfirmed_cpu_models: {% if cloud_mode == 'on' %}['8259C', '8175M', '8259CL', '8171M', '2673', '8370C']{% else %}[]{% endif %} # Update list if required, e.g. ['$0000%@', '8490H']

# CEK project directory on all nodes
project_root_dir: /opt/cek/

{% if vm_mode == 'on' %}
# When vm_recreate_existing is false, existing VMs are not touch during cluster update/scaling
# When vm_recreate_existing is true, existing VMs are destroyed and created again during cluster update/scaling
vm_recreate_existing: false

# When vm_recreate_listed_vms is not empty, listed VMs are destroyed and created again during cluster update/scaling
# even if vm_recreate_existing is false and VMs are in running state
vm_recreate_listed_vms: []
#Example how to use it.
#vm_recreate_listed_vms:
#  - vm-work-1

# When vm_keep_listed_vms is not empty, listed VMs are kept during cluster update/scaling even if they are not running
# It has precedens over vm_recreate_listed_vms. Nevertheless it is overwritten by vm_recreate_existing: true
vm_keep_listed_vms: []

{% endif %}
# Improve deployment stability of Kubespray by increasing wait between retries of failed ops like pushing/downloading
retry_stagger: 20

# Post deployment hooks: hooks_local dir will run .py, .sh and .yaml files (it will find inside this dir) on ansible host
# hooks_remote dir will run .py and .sh scripts (it will find inside this dir) on kube_control_plane;
post_deployment_hook_enabled: false
hooks_local: /root/hooks/local
hooks_remote: /root/hooks/remote

# To deploy only container runtime set this variable as "true", and kubernetes as "false"
# Set both variables as "false" to perform only host configuration
{% if name == 'on_prem_aibox' %}
container_runtime_only_deployment: true
{% else %}
container_runtime_only_deployment: false
{% endif %}

##########################
## System Configuration ##
##########################

# Run system-wide package update (apt dist-upgrade, yum update, ...)
# Note: enabling this may lead to unexpected results
# Tip: you can set this per host using host_vars
update_all_packages: false


{% if arch in ['ultra'] %}
update_kernel: true
nda_kernel_path: "/tmp/linux-kernel-overlay"
nda_firmware_path: "/tmp/firmware"
nda_driver_path: "/tmp/driver"
{% else %}
update_kernel: false
{% endif %}

# Add arbitrary parameters to GRUB
additional_grub_parameters_enabled: false
additional_grub_parameters: ""

# SELinux configuration state: current, enabled, disabled
selinux_state: current

{% if firewall in ['on', 'optional'] %}
firewall_enabled: {% if firewall == "on" %}true{% else %}false{% endif %}


{% endif %}
## Proxy configuration ##
#http_proxy: "http://proxy.example.com:1080"
#https_proxy: "http://proxy.example.com:1080"
#additional_no_proxy: ".example.com,mirror_ip"  # No need to include the following (will be added automatically): localhost, 127.0.0.1, controllerIPs, nodesIPs

# (Ubuntu only) Disables DNS stub listener which may cause issues on Ubuntu
{% if name == 'on_prem_aibox' %}
dns_disable_stub_listener: false
prc_network: false
{% else %}
dns_disable_stub_listener: true
prc_network: false
{% endif %}


# Remove the block between ansible markers set by kubespray in dhclient & hosts files to avoid DNS & LDAP issues (connection loss) after K8s setup after reboot
# It is needed only in some lab environments. Default setting is false.
# TODO: JP: This workaround should not be needed any longer. To be removed completely once it pass complete validation cycle
remove_kubespray_host_dns_settings: false

##############################
## Kubernetes Configuration ##
##############################
{% if name == 'on_prem_aibox' %}
kubernetes: false
{% else %}
kubernetes: true
{% endif %}

# Kubernetes provisioner, Support: rke2(work with os ubuntu22.04 and containerd as container_runtime only), kubespray(default option)
kube_provisioner: kubespray
kube_version: v1.28.3     # test placeholder: n version
#kube_version: v1.27.7   # test placeholder: n-1 version
#kube_version: v1.26.10  # test placeholder: n-2 version
rke2_version: v1.28.3+rke2r1 # test placeholder: n version

{% if kube_dashboard in ['on', 'optional'] %}
# Kubernetes Dashboard
kube_dashboard_enabled: {% if kube_dashboard == 'on' %}true{% else %}false{% endif %}


{% endif %}
# Kubernetes cluster name, also will be used as DNS domain
cluster_name: cluster.local

{% if cert_manager in ['on', 'optional']%}
# Cert manager deployment
cert_manager_enabled: {% if cert_manager == "on"%}true{% else %}false{% endif%}


{% endif %}
# Kubenetes Audit policy custom rules
# https://github.com/kubernetes-sigs/kubespray/blob/master/roles/kubernetes/control-plane/templates/apiserver-audit-policy.yaml.j2
audit_policy_custom_rules: ""

# Kubernetes container runtime: docker, containerd, crio
# When "crio" is set, please enable "crio_registries" section
container_runtime: {{ container_runtime_default }}

{% if rancher_manager in ['on', 'optional'] %}
# Rancher Manager(supported on rke2 currently)
rancher_manager_enabled: {% if rancher_manager == 'on' %}true{% else %}false{% endif %}

{% endif %}

{% if kubevirt in ['on', 'optional'] %}
# Kubevirt - virtual machine management add-on for Kubernetes
# More info here: https://github.com/kubevirt/kubevirt
kubevirt_enabled: {% if kubevirt == 'on' %}true{% else %}false{% endif %}

{% endif %}

########################
## Kubernetes Network ##
########################

kube_controller_manager_bind_address: 127.0.0.1
kube_proxy_metrics_bind_address: 127.0.0.1

# Comment this line out if you want to expose k8s services of type nodePort externally.
kube_proxy_nodeport_addresses_cidr: 127.0.0.0/8

kube_pods_subnet: 10.244.0.0/16
{% if name in ['regional_dc', 'full_nfv', 'access', 'build_your_own', 'base_video_analytics'] %}
{% set mask = 18 %}
{% elif name == 'remote_fp' %}
{% set mask = 19 %}
{% elif name in ['on_prem', 'on_prem_vss', 'on_prem_sw_defined_factory'] %}
{% set mask = 21 %}
{% elif name == 'basic' %}
{% set mask = 22 %}
{% endif %}
kube_service_addresses: 10.233.0.0/{{ mask }}

# Supported plugins: calico, flannel, cilium, cni for kubespray; canal, calico, cilium for rke2
{% if calico_vpp in ['on'] %}
# Currently, calico vpp dataplane is not supported by kubespray,
# so need to install a very basic setup without any actual mesh capable CNI in kubespray.
# Later, install calico and calico vpp dataplane with operator based installations.
kube_network_plugin: cni
{% else %}
kube_network_plugin: calico
{% endif %}

# Calico settings
{% if vm_mode in ['on'] %}
# For VM mode calico_backend has to be vxlan, otherwise deployment will fail
calico_network_backend: vxlan
{% else %}
calico_network_backend: vxlan   # Supported backends: [vxlan, bird(kubespray only)]
{% endif %}

kube_network_plugin_multus: {% if multus == 'on' and calico_vpp != 'on' %}true{% else %}false{% endif %}


# Set on true if you want to enable the eBPF dataplane support
calico_bpf_enabled: false

{% if ingress_nginx in ["on", "optional"] %}
# Kubernetes Ingress Controller to support Ingress resources
# Ingress can be accessed through the nodeport
ingress_enabled: {% if ingress_nginx == 'on' %}true{% else %}false{% endif +%}
# Uncomment if different nodeports are needed to be set
#ingress_nodeport_http: 30123
#ingress_nodeport_https: 30124

{% endif %}
{% if sriov_network_dp in ["on", "optional"] or network_userspace in ["on", "optional"] %}
{% if network_userspace in ['on', 'optional'] %}
# Userspace CNI
userspace_cni_enabled: {% if network_userspace == 'on' %}true{% else %}false{% endif %}


{% endif %}
# Create reference net-attach-def objects
example_net_attach_defs:
  # Values below should match host_vars CNI configuration
{% if sriov_network_dp in ["on", "optional"] %}
  sriov_net_dp: {% if sriov_network_dp == "on" %}true{% else %}false{% endif +%}
{% endif %}
{% if network_userspace in ["on", "optional"] %}
  userspace_ovs_dpdk: {% if ovs_dpdk == "on" %}true{% else %}false{% endif +%}
  userspace_vpp: {% if vpp == "on" %}true{% else %}false{% endif +%}
{% endif %}

{% endif %}
##############################
## Kubernetes Node Features ##
##############################

{% if nfd in ['on', 'optional'] %}
# Node Feature Discovery
nfd_enabled: {% if nfd == 'on' %}true{% else %}false{% endif %}

nfd_namespace: kube-system
nfd_sleep_interval: 60s
{% endif %}

{% if tas in ['on', 'optional'] or gas in ['on', 'optional'] %}
# Intel Platform Aware Scheduling (PAS)
pas_namespace: kube-system

{% if tas in ['on', 'optional'] %}
# Intel Platform Aware Scheduling - Telemetry Aware Scheduling (TAS)
tas_enabled: {% if tas == 'on' %}true{% else %}false{% endif %}

tas_build_image_locally: false
# Create and enable TAS demonstration policy: [true, false]
tas_enable_demo_policy: false
{% endif %}

{% if gas in ['on', 'optional'] %}
# Intel Platform Aware Scheduling - GPU Aware Scheduling (GAS)
gas_enabled: {% if gas == 'on' %}true{% else %}false{% endif %}

gas_build_image_locally: false
{% endif %}
{% endif %}

##################
## CPU Features ##
##################

{% if intel_cpu_controlplane in ['on', 'optional'] %}
# CPU Control Plane Plugin for Kubernetes
# https://github.com/intel/cpu-control-plane-plugin-for-kubernetes
intel_cpu_controlplane:
  enabled: false                  # Enable Intel CPU Control Plane
  allocator: default              # Intel CPU Control plane allocation policy ['default', 'numa', 'numa-namespace', 'numa-namespace-exclusive']
  agent_namespace_prefix: test-   # Intel CPU Control plane agent namespace
  enable_memory_pinning: true     # Enable Intel CPU Control Plane memory pinning

{% endif %}
{% if native_cpu_manager in ['on', 'optional'] %}
# Native CPU Manager (Kubernetes built-in)
# Setting this option as "true" enables the "static" policy, otherwise the default "none" policy is used.
# The reserved CPU cores settings are individual per each worker node, and therefore are available to configure in the host_vars file
native_cpu_manager_enabled: {% if native_cpu_manager == 'on' %}true{% else %}false{% endif %}


{% endif %}
# Kubernetes built-in Topology Manager settings
# There are four supported policies: none, best-effort, restricted, single-numa-node.
topology_manager_policy: "best-effort"
# There are two supported scopes: container, pod
topology_manager_scope: "container"

######################
## Storage Features ##
######################

{% if lpvsp in ['on', 'optional'] or rook_ceph in ['on', 'optional'] or minio in ['on', 'optional'] %}
## Storage simulation configurations ##
# Enable this option, RA uses a file as loop device for storage deployment
# RA create 6 10G loop disk on each node for storage simulation by default.
storage_deploy_test_mode: {% if lpvsp == 'on' or rook_ceph == 'on' or minio == 'on' %}true{% else %}false{% endif %}

storage_nodes: []    #if no setting, all kubenode will be used as storage node.
# storage_nodes:
#   - node0
#   - node1
{% endif %}

{% if lpvsp in ['on', 'optional'] %}
# The local persistence volume static provisioner
# https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner
local_volume_provisioner_enabled: {% if lpvsp == 'on' %}true{% else %}false{% endif %}

local_volume_provisioner_storage_class: "local-static-storage"

{% endif %}
{% if rook_ceph in ['on', 'optional'] %}
## rook_ceph configuration ##
rook_ceph:
  enabled: {% if rook_ceph == 'on' %}true{% else %}false{% endif %}

  storage_class: "rook-cephfs"    # Storage class name
  storage_type: "cephfs"          # Storage type for rook-ceph, supported values[cephfs, nfs, block, object].
  log_level: "DEBUG"              # The logging level for the operator: ["ERROR", "WARNING", "INFO", "DEBUG"]
  allow_loop_devices: true        # Allow using loop devices for osds in test clusters
  enable_nfs: true                # Enable the CSI NFS drivers
  enable_discovery_daemon: true   # Whether to start the discovery daemon to watch for raw stroage devices on nodes in the cluster
  cluster:
    enabled: true                 # Enable sample cluster for a single node
    number_of_mons: 3             # Set the number of mons to be started. Generally recommended to be 3.
                                  # For highest availability, an odd number of mons should be specified.
    allow_multiple_mon_per_node: true
    # The mons should be on unique nodes. For production, at least 3 nodes are recommended for this reason.
    # Mons should only be allowed on the same node for test environments where data loss is acceptable.
    number_of_mgrs: 2             # When higher availability of the mgr is needed, increase the count to 2.
                                  # In that case, one mgr will be active and one in standby. When Ceph updates which
                                  # mgr is active, Rook will update the mgr services to match the active mgr.
    allow_multiple_mgr_per_node: true

{% endif %}
{% if minio in ['on', 'optional'] %}
## MinIO variables ##
# Enable Minio Storage service.
minio_enabled: {% if minio == 'on' %}true{% else %}false{% endif %}

minio_tenant_enabled: true                            # Specifies whether to install MinIO Sample Tenant
minio_tenant_servers: 4                               # The number of MinIO Tenant nodes
minio_tenant_volumes_per_server: 2                    # The number of volumes per servers
minio_tenant_volume_size: 5
                                                      # The size of each volume (unit: GiB)
minio_deploy_test_mode: true                          # When true, use a file as loop device when creating storage
                                                      # called "virtual block device" which is useful for test or automation purpose.
                                                      # When false, use an actual NVME or SSD device when creating storage
minio_build_image_locally: true                       # Build custom MinIO image locally
minio_awsclient_pods_enabled: true                    # Run AWS client pods for MinIO Tenant service

{% endif %}
####################
## Device Plugins ##
####################

{% if sriov_network_dp in ['on', 'optional'] %}
# Intel SRIOV Network Device Plugin
sriov_net_dp_enabled: {% if sriov_network_dp == 'on' %}true{% else %}false{% endif %}

sriov_net_dp_namespace: kube-system
# Whether to build and store image locally or use one from public external registry
sriov_net_dp_build_image_locally: false
# SR-IOV network device plugin configuration.
{% if nic == 'cvl' %}
nic_supported_pf_dev_ids: ["1592", "1593", "159b"]
{% else %}
nic_supported_pf_dev_ids: ["158a", "158b", "1572", "0d58", "1583"]
{% endif %}
nic_supported_vf_dev_ids: ["154c", "10ed", "1889"]
# For more information on supported configuration refer to: https://github.com/intel/sriov-network-device-plugin#configurations
{% if intel_flexran == 'on' %}
# sriovdp_config_data for Intel FlexRAN is defined in the helm_values for the sriov_dp_install role
{% else %}
sriovdp_config_data: |
    {
        "resourceList": [{
                "resourceName": "intel_sriov_netdevice",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["154c", "10ed", "1889"],
                    "drivers": ["iavf", "ixgbevf"]
                }
            },
            {
                "resourceName": "intel_sriov_dpdk_700_series",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["154c", "10ed"],
                    "drivers": ["vfio-pci"]
                }
            },
            {
                "resourceName": "intel_sriov_dpdk_800_series",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"]
                }
{% if name in ['full_nfv', 'access', 'regional_dc', 'build_your_own'] %}
            },
            {
                "resourceName": "intel_fpga",
                "deviceType": "accelerator",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["0d90"]
                }
            }
{% else %}
            }
{% endif %}
        ]
    }
{% endif %}

{% endif %}
{% if sgx_dp in ['on', 'optional'] and arch in ['icx', 'spr', 'emr', 'gnr'] or
    gpu_dp in ['on', 'optional'] or
    qat_dp in ['on', 'optional'] or
    dsa_dp in ['on', 'optional'] and arch in ['spr', 'emr', 'gnr'] or
    dlb_dp in ['on', 'optional'] and arch in ['spr', 'emr', 'gnr'] %}
# Intel Device Plugin Operator
intel_dp_namespace: kube-system   # Namespace will be applied for SGX DP, GPU DP and QAT DP

{% endif %}
{% if dlb_dp in ['on', 'optional'] and arch in ['spr', 'emr', 'gnr'] %}
# Intel Dynamic Load Balancing Device Plugin (Intel DLB DP) for Kubernetes
dlb_dp_enabled: {% if dlb_dp == 'on' %}true{% else %}false{% endif %}     # If true set configure_dlb_devices to true in host vars
dlb_dp_build_image_locally: false
dlb_dp_verbosity: 4

{% endif %}
{% if dsa_dp in ['on', 'optional'] and arch in ['spr', 'emr', 'gnr'] %}
# Intel Data Streaming Accelerator Device Plugin (Intel DSA DP) for Kubernetes
dsa_dp_enabled: {% if dsa_dp == 'on' %}true{% else %}false{% endif %}     # If true set configure_dsa_devices to true in host vars
dsa_dp_build_image_locally: false
dsa_dp_verbosity: 4
dsa_shared_devices: 10    # Number of containers that can share the same DSA device.

{% endif %}
{% if qat_dp in ['on', 'optional'] %}
# Intel QAT Device Plugin for Kubernetes
qat_dp_enabled: {% if qat_dp == 'on' %}true{% else %}false{% endif %}

qat_dp_verbosity: 4
# Maximum number of QAT devices (minimum is 1) to be provided to the QAT Device Plugin.
# To use all available QAT devices on each node, qat_dp_max_devices must be equal to the highest number of QAT Devices from all nodes
# e.g node1 - 48VFs, node2 - 32VFs, qat_dp_max_devices: 48
# It is possible to use a subset of QAT devices in QAT DP. E.g by putting 10 here, QAT DP will use just 10VFs on each node
qat_dp_max_num_devices: 32
qat_dp_build_image_locally: false
# Allocation policy - 2 possible values: balanced and packed.
# Balanced mode spreads allocated QAT VF resources balanced among QAT PF devices, and packed mode packs one QAT PF device
# full of QAT VF resources before allocating resources from the next QAT PF. There is no default value.
#allocation_policy: balanced
qat_supported_pf_dev_ids:
  - "435"
  - "37c8"
  - "19e2"
  - "18ee"
  - "6f54"
  - "18a0"
  - "4940"
  - "4942"
  - "4944"

qat_supported_vf_dev_ids:
  - "443"
  - "37c9"
  - "19e3"
  - "18ef"
  - "6f55"
  - "18a1"
  - "4941"
  - "4943"
  - "4945"

{% endif %}
{% if gpu_dp in ['on', 'optional'] %}
# Intel GPU Device Plugin for Kubernetes
gpu_dp_enabled: {% if gpu == 'on' %}true{% else %}false{% endif %}

gpu_dp_verbosity: 4
gpu_dp_build_image_locally: false

# Configuration-options
# To fully discover the below settings usage, please refer to: https://github.com/intel/intel-device-plugins-for-kubernetes/tree/v0.24.0/cmd/gpu_plugin
gpu_dp_shared_devices: 10           # Number of containers (min. 1) that can share the same GPU device
gpu_dp_monitor_resources: {% if telemetry.intel_xpumanager == 'on' %}true{% else %}false{% endif %}     # Enable monitoring all GPU resources on the node
gpu_dp_fractional_manager: {% if gas == 'on' %}true{% else %}false{% endif %}     # Enable handling of fractional resources for multi-GPU nodes
gpu_dp_prefered_allocation: 'none'  # Available policies are: ['balanced', 'packed', 'none']
{% else %}
gpu_dp_enabled: false
{% endif %}
{% if sgx_dp in ['on', 'optional'] and arch in ['icx', 'spr', 'emr', 'gnr'] %}
# Intel SGX Device Plugin for Kubernetes
sgx_dp_enabled: {% if sgx_dp == 'on' %}true{% else %}false{% endif %}

sgx_dp_verbosity: 4
sgx_dp_build_image_locally: false
sgx_aesmd_namespace: intel-sgx-aesmd
# Deploy SGX AESMD demo app workload
sgx_aesmd_demo_enable: false
# ProvisionLimit is a number of containers that can share the same SGX provision device.
sgx_dp_provision_limit: 20
# EnclaveLimit is a number of containers that can share the same SGX enclave device.
sgx_dp_enclave_limit: 20
{% if vm_mode == 'on' %}
# Memory size for SGX enclave in MB
sgx_memory_size: 16
{% endif %}

{% endif %}

###############
## Operators ##
###############

{% if cloud_mode == 'on' %}
# OpenShift SRIOV Network Operator
# Do not change the value of sriov_network_operator_enabled, as it will cause Cloud RA to fail
sriov_network_operator_enabled: false
{% else %}
{% if sriov_operator in ['on', 'optional'] %}
# OpenShift SRIOV Network Operator
sriov_network_operator_enabled: {% if sriov_operator == 'on' %}true{% else %}false{% endif %}

{% if vm_mode in ['on', 'optional'] %}
# For VM mode sriov_network_operator_enabled has to be false, otherwise VFs
# are not created before VM creation
{% endif %}
sriov_network_operator_namespace: "sriov-network-operator"
{% endif %}

{% endif %}
{% if intel_ethernet_operator.enabled in ['on', 'optional'] %}
# Intel Ethernet Operator for Intel E810 Series network interface cards
intel_ethernet_operator_enabled: {% if intel_ethernet_operator.enabled == 'on' and nic == 'cvl' %}true{% else %}false{% endif %}

# Set to true, if Operator should be build from source, needed for flow_confiration
intel_ethernet_operator_local_build: {% if intel_ethernet_operator.flow_config == 'on' and nic == 'cvl' %}true{% else %}false{% endif %}

# Use together with flow_configuration set in hostvars
intel_ethernet_operator_flow_config_enabled: {% if intel_ethernet_operator.flow_config == 'on' and nic == 'cvl' %}true{% else %}false{% endif %}


{% endif %}
{% if intel_sriov_fec_operator in ['on', 'optional'] %}
# Intel Operator for SR-IOV Wireless Forward Error Correction (FEC) Accelerators
intel_sriov_fec_operator_enabled: {% if intel_sriov_fec_operator == 'on' %}true{% else %}false{% endif %}   # Enable FEC Operator
# When intel_sriov_fec_operator == true and container_runtime == containerd,
# Red Hat Account is needed, refer to https://access.redhat.com/RegistryAuthentication to apply,
# and uncomment below two lines with correct values.
# redhat_user: ffffffffffffffffffffffffffffff
# redhat_password: ffffffffffffffffffffffffffffff

{% endif %}
##################
## Service Mesh ##
##################

{% if istio_service_mesh and istio_service_mesh.enabled in ['on', 'optional'] %}
# Service mesh deployment
# https://istio.io/latest/docs/setup/install/istioctl/
# Intel Istio
# https://github.com/intel/istio

# For all available options, please, refer to 'roles/istio_service_mesh/vars/main.yml'.
# For the options dependencies and compatibility, please, refer to the official CEK documentation.
istio_service_mesh:
  enabled: {% if istio_service_mesh.enabled == 'on' %}true{% else %}false{% endif %}   # Enable Istio Service Mesh
  # Available profiles are: 'default', 'demo', 'minimal', 'external', 'empty', 'preview',
  # 'sgx-mtls', 'intel-qat-hw', 'intel-qat-sw', 'intel-cryptomb'
  # If custom profile needs to be deployed, please, place the file named '<profile_name>.yaml'
  # into the directory 'roles/istio_service_mesh/files/profiles/'.
  # 'custom-ca' profile name is reserved for usage by sgx_signer if sgx_signer option is enabled.
  # Any profile name provided will be overwritten in this case
  profile: {% if istio_service_mesh.sgx_signer == 'on' and arch in ['icx', 'spr'] %}custom-ca{% else %}default{% endif %}  # Istio profile
  intel_preview:
    enabled: {% if istio_service_mesh.intel_preview == 'on' %}true{% else %}false{% endif %}  # Enable intel istio preview
{% if istio_service_mesh.tcpip_bypass_ebpf in ['on', 'optional'] %}
  tcpip_bypass_ebpf:
    enabled: {% if istio_service_mesh.tcpip_bypass_ebpf == 'on' %}true{% else %}false{% endif %}  # Enable tcp/ip ebpf bypass demo
{% endif %}
{% if istio_service_mesh.tls_splicing in ['on', 'optional'] %}
  tls_splicing:
    enabled: {% if istio_service_mesh.tls_splicing == 'on' %}true{% else %}false{% endif %}  # Enable TLS splicing demo
{% endif %}
{% if istio_service_mesh.sgx_signer in ['on', 'optional'] and arch in ['icx', 'spr'] %}
  sgx_signer:
    enabled: {% if istio_service_mesh.sgx_signer == 'on' %}true{% else %}false{% endif %}   # Enable automated key management integration
    name: sgx-signer
{% endif %}
{% if istio_service_mesh.intel_preview in ['on', 'optional'] and arch not in ['emr', 'gnr']%}
  # uncomment following section and enable intel_preview if sgx-mtls profile is selected
  {% if istio_service_mesh.intel_preview == 'optional' %}#{% endif %}set:   # Istio intel preview with sgx-mtls
  {% if istio_service_mesh.intel_preview == 'optional' %}#  {% endif %}- values.global.proxy.sgx.enabled=true   # Istio intel preview with sgx-mtls
  {% if istio_service_mesh.intel_preview == 'optional' %}#  {% endif %}- values.global.proxy.sgx.certExtensionValidationEnabled=true   # Istio intel preview with sgx-mtls
  {% if istio_service_mesh.intel_preview == 'optional' %}#  {% endif %}- values.gateways.sgx.enabled=true   # Istio intel preview with sgx-mtls
  {% if istio_service_mesh.intel_preview == 'optional' %}#  {% endif %}- values.gateways.sgx.certExtensionValidationEnabled=true   # Istio intel preview with sgx-mtls
{% endif %}

{% endif %}
{% if linkerd_service_mesh and linkerd_service_mesh.enabled in ['on', 'optional'] %}
# LinkerD service mesh
# https://linkerd.io/
linkerd_service_mesh:
  enabled: {% if linkerd_service_mesh.enabled == 'on' %}true{% else %}false{% endif %}   # Enable LinkerD Service Mesh

{% endif %}
###############################
## Telemetry & Observability ##
###############################

{% if telemetry.collectd in ['on', 'optional'] %}
# Collectd is a daemon which collects system information and provides mechanisms to store and monitor the values in a variety of ways.
# If Collectd is enabled then Telegraf must be disabled.
collectd_enabled: {% if telemetry.collectd == 'on'%}true{% else %}false{% endif +%}
collectd_scrape_interval: 30

{% endif %}
{% if telemetry.telegraf in ['on', 'optional'] %}
# Telegraf is an agent for collecting, processing, aggregating, and writing metrics.
# If Telegraf is enabled then Collectd must be disabled.
telegraf_enabled: {% if telemetry.telegraf == 'on'%}true{% else %}false{% endif +%}
telegraf_scrape_interval: 30

{% endif %}
{% if telemetry.jaeger in  ['on', 'optional'] %}
# Jaeger is a distributed tracing platform that can be used for monitoring microservices-based distributed systems.
# Jaeger in RA sends tracing telemetry data to ElasticSearch, therefore elasticsearch must be enabled as well.
jaeger_enabled: {% if telemetry.jaeger == 'on'%}true{% else %}false{% endif +%}

{% endif %}
{% if cadvisor in ['on', 'optional'] %}
# cAdvisor provides container users an understanding of the resource usage and performance characteristics of their running containers.
# It is a running daemon that collects, aggregates, processes, and exports information about running containers
cadvisor_enabled: {% if cadvisor == 'on' %}true{% else %}false{% endif +%}
# Enablement of scraping specific CPU perf events
cadvisor_sample_perf_events_enabled: false
cadvisor_pik_perf_events_enabled: false

{% endif %}
{% if telemetry.opentelemetry in ['on', 'optional'] %}
# Opentelemetry collectors are used to scrap metrics from telegraf and cAdvisor and pass them to elasticsearch and prometheus
# If Opentelemetry is enabled, prometheus, jaeger and elasticsearch must be enabled as well.
opentelemetry_enabled: {% if telemetry.opentelemetry == 'on'%}true{% else %}false{% endif +%}

{% endif %}
{% if telemetry.prometheus in  ['on', 'optional'] %}
# Prometheus stack includes prometheus, node_exporter and grafana deployments.
prometheus_stack_enabled: {% if telemetry.prometheus == 'on'%}true{% else %}false{% endif +%}

{% endif %}
{% if telemetry.elasticsearch in ['on', 'optional'] %}
# Elasticsearch ECK is a distributed, RESTful search and analytics engine.
eck_enabled: {% if telemetry.elasticsearch == 'on'%}true{% else %}false{% endif +%}

{% endif %}
{% if telemetry.kibana in ['on', 'optional'] %}
# Kibana is used to visualize data from elasticsearch.
# If Kibana is enabled, elasticsearch must be enabled as well.
kibana_enabled: {% if telemetry.kibana == 'on'%}true{% else %}false{% endif +%}

{% endif %}
{% if telemetry.intel_xpumanager in ['on', 'optional'] %}
# intel_xpumanager plugin collects information about Intel data center GPUs.
# If xpumanager is enabled, Prometheus stack must be enabled as well.
intel_xpumanager_enabled: {% if telemetry.intel_xpumanager == 'on'%}true{% else %}false{% endif +%}

{% endif %}

######################
## Power Management ##
######################
{% if power.manager in ['on', 'optional'] and arch in ['icx', 'clx', 'spr', 'emr', 'gnr'] %}
# Kubernetes Power Manager
kubernetes_power_manager:
  enabled: {% if power.manager == 'on' %}true{% else %}false{% endif %} # Enable/Disable power manager

  power_nodes: []                                                         # List of power_nodes that should be considered during Operator work and profiles deployment
    # - node1
    # - node2

  build_image_locally: true                                              # Build Power Manager image locally
  deploy_example_pods: true                                              # Deploy example Pods that will utilize special resources
  global_shared_profile_enabled: true                                    # Deploy custom Power Profile with user defined frequencies that can be applied to all power nodes
                                                                          # to make use of Shared Profile fill Shared Workload settings in host vars
  global_max_frequency: 1500                                              # Max frequency that will be applied for cores by Shared Workload
  global_min_frequency: 1000                                              # Min frequency that will be applied for cores by Shared Workload

{% if power.frequency_scaling in ['on', 'optional'] and arch in ['icx', 'clx', 'spr', 'emr', 'gnr'] %}
  # !Please set up scaling driver in host_vars.yml file!
  # available governors:
  # "powersave" - Lowest frequency within the borders of min_frequency and max_frequency.
  # "performance" - Highest frequency within the borders of min_frequency and max_frequency.
  # "userspace" - !CPUFREQ ONLY! - Allow user space to set CPU frequency in scaling_setspeed attribute
  # "schedutil" - !CPUFREQ ONLY! - Uses data from CPU scheduler to set up frequency
  global_governor: "powersave"

{% endif %}
{% endif %}
{% if infra_power_manager in ['on', 'optional'] %}
# Warning: currently there's no support to deploy IPM.
# Setting this parameter to true will only apply required DPDK patches provided by IPM.
infrastructure_power_manager_enabled: {% if infra_power_manager == 'on' %}true{% else %}false{% endif %}


{% endif %}

##############################
## Security & Certification ##
##############################

{% if openssl in ['on', 'optional'] %}
# This feature will enable OpenSSL*Engine
openssl_engine_enabled: {% if openssl == 'on' %}true{% else %}false{% endif %} # to activate OpenSSL*Engine set 'install_openssl' to 'true' in host_vars

{% endif %}
{% if (kmra and (kmra.pccs in ['on', 'optional'] or
    kmra.apphsm in ['on', 'optional'] or
    kmra.ctk_demo in ['on', 'optional'] or
    kmra.oran in ['on', 'optional'])) and
    arch in ['icx', 'spr'] %}
# KMRA (Key Management Reference Application)
# Please, refer to the roles/kmra_install/defaults/main.yml for the full list of configuration options available.
kmra:
{% if kmra.sbx in ['on', 'optional'] %}
  sbx: {% if kmra.sbx == 'on' %}true{% else %}false{% endif %} # Enable pre-PRQ SGX platform
{% endif %}
{% if kmra.oran in ['on', 'optional'] %}
  oran:
    enabled: {% if kmra.oran == 'on' %}true{% else %}false{% endif %} # Put KMRA into ORAN mode
    local_build: {% if kmra.oran == 'on' %}true{% else %}false{% endif %} # Build oran container by default unless you have pre-built oran container
  oran_netopeer2_server:
    enabled: {% if kmra.oran == 'on' %}true{% else %}false{% endif %} # Enable netopeer2 server
  oran_netopeer2_client:
    enabled: {% if kmra.oran == 'on' %}true{% else %}false{% endif %} # Enable netopeer2 client
{% endif %}
{% if kmra.pccs in ['on', 'optional'] %}
  pccs:
    enabled: {% if kmra.pccs == 'on' %}true{% else %}false{% endif %}   # Enable PCCS application
    # The PCCS uses this API key to request collaterals from Intel's Provisioning Certificate Service. User needs to subscribe first to obtain an API key.
    # For how to subscribe to Intel Provisioning Certificate Service and receive an API key, go to https://api.portal.trustedservices.intel.com/provisioning-certification,
    # and get an API key by clicking 'Subscribe'.
    api_key: "ffffffffffffffffffffffffffffffff"
{% endif %}
{% if kmra.apphsm in ['on', 'optional'] %}
  apphsm:
    enabled: {% if kmra.apphsm == 'on' %}true{% else %}false{% endif %}   # Enable AppHSM application
{% endif %}
{% if kmra.ctk_demo in ['on', 'optional'] %}
  ctk_loadkey_demo:
    enabled: {% if kmra.ctk_demo == 'on' %}true{% else %}false{% endif %}   # Enable CTK demo application
{% endif %}

{% endif %}
{% if tcs in ['on', 'optional'] and
    arch in ['icx', 'spr'] %}
# Trusted Certificate Service deployment
# https://github.com/intel/trusted-certificate-issuer
tcs:
  enabled: {% if tcs == 'on' %}true{% else %}false{% endif %}   # Enable Trusted Certificate Issuer
  build_image_locally: false   # Build Trusted Certificate Issuer image locally

{% endif %}
{% if tac in ['on', 'optional'] and
    arch in ['icx', 'spr'] %}
# Trusted Attestation Controller deployment
# https://github.com/intel/trusted-attestation-controller
tac:
  enabled: {% if tac == 'on' %}true{% else %}false{% endif %}   # Enable Trusted Attestation Controller
  build_image_locally: false   # Build Trusted Attestation Controller image locally

{% endif %}
{% if sigstore_policy_controller in ['on', 'optional'] %}
# Install sigstore policy controller to enforce cosign container image security
sigstore_policy_controller_install: {% if sigstore_policy_controller == 'on' %}true{% else %}false{% endif %}


{% endif %}
{% if intel_media_analytics in ['on', 'optional'] %}
####################
## Video AI / VSS ##
####################

intel_media_analytics_enabled: {% if intel_media_analytics == 'on' %}true{% else %}false{% endif %}


{% endif %}
{% if intel_oneapi and (intel_oneapi.values() | reject('eq', 'off') | list | length() != 0) %}
###########################
## Intel OneAPI Toolkits ##
###########################
intel_oneapi_enabled: {% if (intel_oneapi.values() | select('eq', 'on')) | list | length() > 0 %}true{% else %}false{% endif %} # enable Intel oneAPI toolkits deployment
intel_oneapi:
{%- if intel_oneapi.base in ['on', 'optional'] +%}
  # Set to true to deploy Intel oneAPI Base Kit
  basekit: {% if intel_oneapi.base == 'on' %}true{% else %}false{% endif %}
{% endif %}
{%- if intel_oneapi.ai in ['on', 'optional'] +%}
  # Set to true to deploy Intel oneAPI AI Analytics Kit
  ai_analytics: {% if intel_oneapi.ai == 'on' %}true{% else %}false{% endif %}
{% endif %}


{% endif %}

{% if imtl in ['on', 'optional'] %}
######################
## System Libraries ##
######################

# IMTL: DPDK-based solution designed for high-throughput, low-latency transmission and reception of media data
# Supported only for Intel® E810 Series Network Cards!
# Source: https://github.com/OpenVisualCloud/Media-Transport-Library
intel_media_transport_library_enabled: {% if imtl == "on" %}true{% else %}false{% endif %} # enables IMTL deployment
intel_media_transport_library:
  # Patch of ICE driver is needed, set to false if ICE driver is already patched
  # update_nic_drivers option in host_vars must be set to true in order to patch, build and load ICE driver
  patch_nic_driver: true

{% endif %}

##########################
## Container Registries ##
##########################

{% if registry in ['on', 'optional'] %}
# Docker registry running on the cluster allows us to store images not available on Docker Hub
registry_enable: {% if registry == 'on' %}true{% else %}false{% endif %}

registry_nodeport: "30500"  # The range of valid ports is 30000-32767
registry_local_address: "localhost:{{ '{{' }} registry_nodeport {{ '}}' }}"

{% endif %}
# Set image pull policy to Always. Pull images prior to starting containers. Valid credentials must be configured.
always_pull_enabled: false

# Registry mirrors can be configured using the following options
#docker_registry_mirrors:
#  - http://mirror_ip:mirror_port
#docker_insecure_registries:
#  - http://docker_insecure_registry_ip
#containerd_registries_mirrors:
#  - prefix: docker.io
#    mirrors:
#      - host: https://registry-1.docker.io
#        capabilities: ["pull", "resolve"]
#        skip_verify: false
#      - host: https://containerd_insecure_registry
#        capabilities: ["pull", "resolve"]
#        skip_verify: true
#crio_registries:
#  - prefix: docker.io
#    insecure: false
#    blocked: false
#    location: registry-1.docker.io
#    unqualified: true
#    mirrors:
#      - location: mirror_ip:mirror_port
#        insecure: false
#crio_insecure_registries:
#  - http://crio_insecure_registry_ip

#######################
## Workloads & Demos ##
#######################

{% if rt_kernel in ['on', 'optional'] %}
# Realtime kernel
rt_kernel_enabled: {% if rt_kernel == 'on' %}true{% else %}false{% endif %} # if true, install realtime kernel
ubuntu_pro_token: "ffffffffffffffffffffffffffffff" # need to attach Ubuntu Pro free token to download RT kernel(please apply it firstly)
{% endif %}

{% if intel_flexran in ['on', 'optional'] %}
# Intel FlexRAN
intel_flexran_enabled: {% if intel_flexran == 'on' %}true{% else %}false{% endif %}    # Enable deployment of FlexRAN
intel_flexran_type: "host"       # Supported values are "host" and "pod"
intel_flexran_mode: "timer"     # Supported values are "timer" and "xran"
# The below 4 values must be strings in extended Bus:Device.Function (BDF) notation
intel_flexran_bbu_front_haul: "0000:43:00.0"
intel_flexran_bbu_ptp_sync: "0000:43:00.1"
intel_flexran_oru_front_haul: "0000:4b:00.0"
intel_flexran_oru_ptp_sync: "0000:4b:00.1"

{% endif %}
{% if tadk in ['on', 'optional'] %}
# Traffic Analytics Development Kit (TADK)
tadk_install: {% if tadk == 'on' %}true{% else %}false{% endif %}   # Install Web Application Firewall (WAF) using TADK

{% endif %}
{% if adq_dp in ['on', 'optional'] %}
# Application Device Queues (ADQ)
# ADQ is experimental feature and enabling it may lead to unexpected results.
# ADQ requires back-to-back connection between control and worker nodes on CVL interfaces.
# Name of CVL interfaces must be the same on both nodes, IP address must be present.
# In inventory.ini set "ip=" to IP address of CVL interface.
# Additional requirements and details can be found in docs/adq.md
adq_dp:
  enabled: false
  # IP address of CVL interface located on the control plane
  interface_address: "192.168.0.10"
  interface_name: "ens107"
{% endif %}

{% if calico_vpp in ['on', 'optional'] %}
# Calico VPP dataplane is in beta and should not be used in production clusters.
# It requires the used interface must be the name of a Linux interface, up and configured with an address.
# The address configured on this interface(e.g. CVL interface) must be the node address in Kubernetes.
# In inventory.ini set "ip=" to IP address of CVL interface.
# Additional requirements and details can be found in docs/calico_vpp.md
calico_vpp:
  enabled: {% if calico_vpp == 'on' %}true{% else %}false{% endif %} 
  interface_name: "enxcalicovpp001"
{% endif %}

{% if intel_eci and (intel_eci.values() | reject('eq', 'off')) | list | length() > 0 %}
# Please contact eci-support@intel.com on how to access this repo.
# Also refer to ESH (https://www.intel.com/content/www/us/en/edge-computing/edge-software-hub.html)
intel_eci_repo: <link_to_eci_apt_repo_from_ESH>
{% endif %}

{% if intel_csl_excat in ['on', 'optional'] %}
# Please contact eci-support@intel.com on how to get the csl_excat release
# Also refer to ESH (https://www.intel.com/content/www/us/en/edge-computing/edge-software-hub.html)
intel_csl_excat_enabled: {% if intel_csl_excat == 'on' %}true{% else %}false{% endif %}


{% endif %}
{% if ido in ['on', 'optional'] %}
# Intent Driven Orchestration (IDO)
# Installs IDO and the linkerd-viz extension, with optional support for deploying a demo workload.
# Follows the installation steps in https://github.com/intel/intent-driven-orchestration
# Requirements: LinkerD and local Docker registry must be enabled
ido:
  enabled: {% if ido == 'on' %}true{% else %}false{% endif %}          # Installs IDO and linkerd-viz extension
  demo_workload: false    # Installs 'ido-example-deployment' and IDO intent linked to workload.
                          # IDO can be tested by changing the number of replicas to 5+
                          # After a few minutes the number of replicas should decrease towards 1.

{% endif %}
{% if mirrors == 'true' %}
########################
## Repository Mirrors ##
########################

## Kubespray image repositories mirrors
gcr_image_repo: "gcr.m.daocloud.io"
kube_image_repo: "k8s.m.daocloud.io"
docker_image_repo: "docker.m.daocloud.io"
quay_image_repo: "quay.m.daocloud.io"
github_image_repo: "ghcr.m.daocloud.io"

## Kubespray pkg repositories mirrors
docker_rh_repo_base_url: "https://myinternalyumrepo/docker-ce/$releasever/$basearch"
docker_rh_repo_gpgkey: "https://myinternalyumrepo//docker-ce/gpg"
docker_ubuntu_repo_base_url: "https://myinternalubunturepo/docker-ce"
docker_ubuntu_repo_gpgkey: "https://myinternalubunturepo/docker-ce/gpg"
containerd_ubuntu_repo_base_url: "https://myinternalubunturepo/containerd"
containerd_ubuntu_repo_gpgkey: "https://myinternalubunturepo/containerd/gpg"
containerd_ubuntu_repo_repokey: 'YOURREPOKEY'

## Kubespray url mirrros
mirror_urls:
  - original: "https://storage.googleapis.com"
    mirror: "https://googleapis.daocloud.io"  # Please set mirror url
  - original: "https://github.com"
    mirror: "https://github.daocloud.io"      # Please set mirror url
  - original: "https://get.helm.sh"
    mirror: "https://helm.daocloud.io"        # Please set mirror url

{% endif %}

{% if intel_ffmpeg in ['on', 'optional'] %}
## Install FFmpeg with custom patches
# ffmpeg_patches: # List of patch resources to apply
#   - url: # URL of archive or git repository
#     type: # Type of source ["tar.gz", "zip", "git"]
#     sha256: # SHA needed for successful deployment, generate from file
#     subdirectory: # Where to search for patches inside repository (for root directory use '/')
#     git_tag: # Use when type is set to git
#     patchset_enabled: true # [true/false] If true, this set of patches will be applied in the FFmpeg code, if false patches are skipped
#     apply_all_patches: # If true script will apply all patches from defined subdirectory,
#                          if false script will apply patches from list bellow
#     patches_to_apply: # Used only when "apply_all_patches" is 'false'
#       - patch_1
#       - patch_2

ffmpeg_install_enabled: {% if intel_ffmpeg == 'on' %}true{% else %}false{% endif %}

ffmpeg_patches:
  - url: "https://github.com/intel/cartwheel-ffmpeg/archive/refs/tags/2023q3.tar.gz"
    type: "tar.gz"
    sha256: "6d85524b99cc056b0823397d2b6f06e4375f61f59218b97b7b4713ba12739ae6"
    subdirectory: "patches/"
    patchset_enabled: true
    apply_all_patches: true

{% endif %}

{% if base_container in ['on', 'optional'] %}
intel_base_container_enabled: {% if base_container == 'on' %}true{% else %}false{% endif %}

{% endif %}

{% if name == 'on_prem_vss' %}
build_base_images: true
{% else %}
build_base_images: false
{% endif %}

{% if inbm in ['on', 'optional'] %}
intel_inband_manageability_enabled: {% if inbm == 'on' %}true{% else %}false{% endif %}

# Supported values for mode are 'inbc', 'cloud'.
# If local inbc option is chosen then provisioning will be performed automatically, otherwise provisioning should be run manually using provision-tc command.
# For more information please refer to
# https://github.com/intel/intel-inb-manageability/blob/develop/docs/In-Band%20Manageability%20Installation%20Guide%20Ubuntu.md
intel_inband_manageability_mode: 'inbc'
{% endif %}
