---
## Container Experience Kits (CEK) primary playbook variables ##
# Do not change profile_name and configured_arch here !!!
# To generate vars for different profile/architecture use make command
# generated for profile and arch:
profile_name: {{ name }}
configured_arch: {{ arch }}

# CEK project directory on all nodes
project_root_dir: /opt/cek/

vm_enabled: {% if vm_mode == 'on' %}true{% else %}false{% endif %}
{%- if vm_mode in ['optional'] %}
# vm_mode can't be enabled manualy here
# to enable it, vm specific configuration from examples/vm need to be taken
{%- endif %}

# Kubernetes version
kubernetes: true
kube_version: v1.22.3
#kube_version: v1.21.5
#kube_version: v1.20.6
# To deploy only container runtime set this variable as "true", and kubernetes as "false"
# set both variables as "false" to perform only host configuration
container_runtime_only_deployment: false

# Kubernetes container runtime: docker, containerd, crio
container_runtime: docker

# Preflight will check vars configuration
# It is NOT recommended to disable preflight, unless it is a conscious decision
preflight_enabled: true

# Run system-wide package update (apt dist-upgrade, yum update, ...)
# Note: enabling this may lead to unexpected results
# Tip: you can set this per host using host_vars
update_all_packages: false
update_kernel: false

# Add arbitrary parameters to GRUB
additional_grub_parameters_enabled: false
additional_grub_parameters: ""

# SELinux configuration state: current, enabled, disabled
selinux_state: current
{% if nfd in ['on', 'optional'] %}
# Node Feature Discovery
nfd_enabled: {% if nfd == 'on' %}true{% else %}false{% endif %}
nfd_build_image_locally: false
nfd_namespace: kube-system
nfd_sleep_interval: 60s
{% endif %}

{%- if native_cpu_manager in ['on', 'optional'] %}
# Native CPU Manager (Kubernetes built-in)
# Setting this option as "true" enables the "static" policy, otherwise the default "none" policy is used.
# The reserved CPU cores settings are individual per each worker node, and therefore are available to configure in the host_vars file
native_cpu_manager_enabled: {% if native_cpu_manager == 'on' %}true{% else %}false{% endif %}
{% endif %}
{% if topology_manager in ['on', 'optional'] -%}
# Enable Kubernetes built-in Topology Manager
topology_manager_enabled: {% if topology_manager == 'on' %}true{% else %}false{% endif %}
# There are four supported policies: none, best-effort, restricted, single-numa-node.
topology_manager_policy: "best-effort"
{% endif %}

{%- if sriov_operator in ['on', 'optional'] %}
# OpenShift SRIOV Network Operator
sriov_network_operator_enabled: {% if sriov_operator == 'on' %}true{% else %}false{% endif %}
{%- if vm_mode in ['on', 'optional'] %}
# For VM mode sriov_network_operator_enabled has to be false, otherwise VFs
# are not created before VM creation
{%- endif %}
sriov_network_operator_namespace: "sriov-network-operator"
{% endif %}

{%- if sriov_network_dp in ['on', 'optional'] %}
# Intel SRIOV Network Device Plugin
sriov_net_dp_enabled: {% if sriov_network_dp == 'on' %}true{% else %}false{% endif %}
sriov_net_dp_namespace: kube-system
# whether to build and store image locally or use one from public external registry
sriov_net_dp_build_image_locally: false
# SR-IOV network device plugin configuration.
# For more information on supported configuration refer to: https://github.com/intel/sriov-network-device-plugin#configurations
sriovdp_config_data: |
    {
        "resourceList": [{
                "resourceName": "intel_sriov_netdevice",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["154c", "10ed", "1889"],
                    "drivers": ["iavf", "ixgbevf"]
                }
            },
            {
                "resourceName": "intel_sriov_dpdk_700_series",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["154c", "10ed"],
                    "drivers": ["vfio-pci"]
                }
            },
            {
                "resourceName": "intel_sriov_dpdk_800_series",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"]
                }
            {% if name in ['full_nfv', 'access', 'regional_dc'] -%}
            },
            {
                "resourceName": "intel_fpga",
                "deviceType": "accelerator",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["0d90"]
                }
            }
            {%- else -%}
            }
            {%- endif %}
        ]
    }
{% endif %}

{%- if power_manager in ['on', 'optional'] and arch in ['icx', 'clx', 'spr'] %}
# Intel Kubernetes Power Manager
intel_power_manager:
    enabled: {% if power_manager == 'on' %}true{% else %}false{% endif %}
    # The performance profile is available for nodes that has CPU max MHz > 3500.0000 - use 'lscpu' command to see your node details
    power_profiles: [performance, balance-performance, balance-power]       # the list of PowerProfiles that will be available on the nodes
                                                                            # possible PowerProfiles are: performance, balance_performance, balance_power
    power_nodes: []                                                         # list of nodes that should be considered during Operator work and profiles deployment
    build_image_locally: false
    deploy_example_pods: false                                      # deploy example Pods that will utilize special resources
    global_shared_profile_enabled: false                            # deploy custom Power Profile with user defined frequencies that can be applied to all power nodes
                                                                    # to make use of Shared Profile fill Shared Workload settings in host vars
    max_shared_frequency: 1500                                      # max frequency that will be applied for cores by Shared Workload
    min_shared_frequency: 1000                                      # min frequency that will be applied for cores by Shared Workload
{% endif %}

{%- if sgx_dp in ['on', 'optional'] and arch in ['icx', 'spr'] or
    gpu in ['on', 'optional'] and arch in ['icx', 'clx', 'spr'] or
    qat_dp in ['on', 'optional'] %}
# Intel Device Plugin Operator
intel_dp_namespace: kube-system # namespace will be applied for SGX DP, GPU DP and QAT DP
{% endif %}

{%- if qat_dp in ['on', 'optional'] %}
# Intel QAT Device Plugin for Kubernetes
qat_dp_enabled: {% if qat_dp == 'on' %}true{% else %}false{% endif %}
qat_dp_verbosity: 4
# Maximum number of QAT devices (minimum is 1) to be provided to the QAT Device Plugin.
# To use all available QAT devices on each node, qat_dp_max_devices must be equal to the highest number of QAT Devices from all nodes
# e.g node1 - 48VFs, node2 - 32VFs, qat_dp_max_devices: 48
# It is possible to use a subset of QAT devices in QAT DP. E.g by putting 10 here, QAT DP will use just 10VFs on each node
qat_dp_max_num_devices: 32
qat_dp_build_image_locally: {% if vm_mode == 'on' %}true{% else %}false{% endif %}

qat_supported_pf_dev_ids:
  - "435"
  - "37c8"
  - "19e2"
  - "18ee"
  - "6f54"
  - "18a0"
  - "4940"

qat_supported_vf_dev_ids:
  - "443"
  - "37c9"
  - "19e3"
  - "18ef"
  - "6f55"
  - "18a1"
  - "4941"
{% endif %}

{%- if openssl in ['on', 'optional'] %}
# This feature will enable OpenSSL*Engine
openssl_engine_enabled: {% if openssl == 'on' and qat == 'on' %}true{% else %}false{% endif %} # To activate OpenSSL*Engine set both install_openssl and update_qat_drivers to 'true' in host_vars
{% endif %}

{%- if gpu in ['on', 'optional'] and arch in ['icx', 'clx', 'spr'] %}
# Intel GPU Device Plugin for Kubernetes
gpu_dp_enabled: {% if gpu == 'on' %}true{% else %}false{% endif %}
gpu_dp_kernel_version: "5.4.48+"
gpu_dp_verbosity: 4
gpu_dp_build_image_locally: false

# Please refer to: https://github.com/intel/intel-device-plugins-for-kubernetes/tree/v0.23.0/cmd/gpu_plugin#configuration-options
# to fully discover the below settings usage
gpu_dp_shared_devices: 10  # number of containers (min. 1) that can share the same GPU device
gpu_dp_monitor_resources: false  # enable monitoring all GPU resources on the node
gpu_dp_fractional_manager: false  # enable handling of fractional resources for multi-GPU nodes
gpu_dp_prefered_allocation: 'none'  # available policies are: ['balanced', 'packed', 'none']

# For systems with older kernel drivers or GPUs which do not support reading the GPU memory amount,
# the gpu_dp_max_memory variable value is turned into a GPU memory amount label instead of a read value.
# Please set carefully as this will be set as the maximum available memory of GPU on the nodes!
# If GPU will be capable of manifest its size via the driver this variable will be ignored.
gpu_dp_max_memory: "8 GB" # max memory per card - for Intel SG1 single card has 8 GB of memory
{% endif %}

{%- if sgx_dp in ['on', 'optional'] and arch in ['icx', 'spr'] %}
# Intel SGX Device Plugin for Kubernetes
sgx_dp_enabled: {% if sgx_dp == 'on' %}true{% else %}false{% endif %}
sgx_dp_verbosity: 4
sgx_dp_build_image_locally: false
sgx_aesmd_namespace: kube-system
# ProvisionLimit is a number of containers that can share
# the same SGX provision device.
sgx_dp_provision_limit: 20
# EnclaveLimit is a number of containers that can share the
# same SGX enclave device.
sgx_dp_enclave_limit: 20
{% endif %}

{%- if kmra in ['on', 'optional'] and arch in ['icx', 'spr'] %}
# KMRA (Key Management Reference Application)
kmra_enabled: {% if kmra == 'on' %}true{% else %}false{% endif %}
# The PCCS uses this API key to request collaterals from Intel's Provisioning Certificate Service.
# User needs to subscribe first to obtain an API key.
# For how to subscribe to Intel Provisioning Certificate Service and receive an API key,
# goto https://api.portal.trustedservices.intel.com/provisioning-certification and click on 'Subscribe'.
kmra_pccs_api_key: "ffffffffffffffffffffffffffffffff"
# deploy KMRA demo workload (NGINX server)
kmra_deploy_demo_workload: true
{% endif %}

{%- if service_mesh in ['on', 'optional'] %}
# Service mesh deployment
# https://istio.io/latest/docs/setup/install/istioctl/

# for all available options, please, refer to the 'roles/service_mesh_install/vars/main.yml;
service_mesh:
  enabled: {% if service_mesh == 'on' %}true{% else %}false{% endif %}
  # available profiles are: 'default', 'demo', 'minimal', 'external', 'empty', 'preview'
  # if custom profile needs to be deployed, please, place the file named '<profile_name>.yaml'
  # into the directory 'roles/service_mesh_install/files/profiles/'
  profile: default
  tcpip_bypass_ebpf:
    enabled: true
{% endif %}

{%- if tas in ['on', 'optional'] or gas in ['on', 'optional'] %}
# Intel Platform Aware Scheduling (PAS)
pas_namespace: kube-system

{%- if tas in ['on', 'optional'] %}
# Intel Platform Aware Scheduling - Telemetry Aware Scheduling (TAS)
tas_enabled: {% if tas == 'on' %}true{% else %}false{% endif %}
tas_build_image_locally: false
# create and enable TAS demonstration policy: [true, false]
tas_enable_demo_policy: false
{% endif %}

{%- if gas in ['on', 'optional'] %}
# Intel Platform Aware Scheduling - GPU Aware Scheduling (GAS)
gas_enabled: {% if gas == 'on' %}true{% else %}false{% endif %}
gas_build_image_locally: false
{%- endif %}
{%- endif %}

# Telemetry configuration. Collectd and Telegraf variables are mutually exclusive.
collectd_enabled: false
telegraf_enabled: true
collectd_scrap_interval: 30
telegraf_scrap_interval: 30
{% if sriov_network_dp in ["on", "optional"] or network_userspace in ["on", "optional"] %}
# Create reference net-attach-def objects
example_net_attach_defs:
{%- if sriov_network_dp in ["on", "optional"] %}
  sriov_net_dp: {% if sriov_network_dp == "on" %}true{% else %}false{% endif %} # Update to match host_vars CNI configuration
{%- endif -%}
{%- if network_userspace in ["on", "optional"] %}
  userspace_ovs_dpdk: {% if network_userspace == "on" %}true{% else %}false{% endif %} # Update to match host_vars CNI configuration
  userspace_vpp: false # Update to match host_vars CNI configuration
{%- endif %}
{%- endif %}
{% if firewall in ['on', 'optional'] %}
firewall_enabled: {% if firewall == "on" %}true{% else %}false{% endif %}
{%- endif %}

## Proxy configuration ##
#http_proxy: "http://proxy.example.com:1080"
#https_proxy: "http://proxy.example.com:1080"
#additional_no_proxy: ".example.com,mirror_ip"

# (Ubuntu only) disables DNS stub listener which may cause issues on Ubuntu
dns_disable_stub_listener: true

# Kubernetes cluster name, also will be used as DNS domain
cluster_name: cluster.local

## Kubespray variables ##

kube_controller_manager_bind_address: 127.0.0.1
kube_proxy_metrics_bind_address: 127.0.0.1
# supported network plugins(calico, flannel) and kube-proxy configuration
kube_network_plugin: calico
# supported calico backend: [vxlan, bird]
{%- if vm_mode in ['on'] %}
calico_backend: vxlan
# For VM mode calico_backend has to be vxlan, otherwise deployment will fail
{%- else %}
calico_backend: bird
{%- endif %}
wireguard_enabled: {% if wireguard == 'on' %}true{% else %}false{% endif %}
kube_network_plugin_multus: true
kube_pods_subnet: 10.244.0.0/16
{%- if name in ['regional_dc', 'full_nfv', 'access', 'storage'] -%}
{% set mask = 18 %}
{%- elif name == 'remote_fp' -%}
{% set mask = 19 %}
{%- elif name == 'on_prem' -%}
{% set mask = 21 %}
{%- elif name == 'basic' -%}
{% set mask = 22 %}
{%- endif %}
kube_service_addresses: 10.233.0.0/{{ mask }}
kube_proxy_mode: iptables

#set both below on true if you want to enable the eBPF dataplane support
calico_bpf_enabled: false
kube_proxy_remove: false

# Set this var to true if you want to expose calico metrics endpoint
calico_metrics_enabled: false

# comment this line out if you want to expose k8s services of type nodePort externally.
kube_proxy_nodeport_addresses_cidr: 127.0.0.0/8

# local Docker Hub mirror, if it exists
#docker_registry_mirrors:
#  - http://mirror_ip:mirror_port
#docker_insecure_registries:
#  - http://docker_insecure_registry_ip
#containerd_registries:
#  "docker.io":
#    - "https://registry-1.docker.io"
#    - "http://mirror_ip:mirror_port"
#crio_registries_mirrors:
#  - prefix: docker.io
#    insecure: false
#    blocked: false
#    location: registry-1.docker.io
#    mirrors:
#      - location: mirror_ip:mirror_port
#        insecure: false
#crio_insecure_registries:
#  - http://crio_insecure_registry_ip

# Docker registry running on the cluster allows us to store images not avaialble on Docker Hub
# The range of valid ports is 30000-32767
registry_nodeport: 30500
registry_local_address: "localhost:{{ '{{' }} registry_nodeport {{ '}}' }}"

# Enable Pod Security Policy. This option enables PSP admission controller and creates minimal set of rules.
psp_enabled: true

# Set image pull policy to Always. Pulls images prior to starting containers. Valid credentials must be configured.
always_pull_enabled: false

{%- if minio in ['on', 'optional'] %}
## MinIO variables ##
# Enable Minio Storage service.
minio_enabled: {% if minio == 'on' %}true{% else %}false{% endif %}

minio_tenant_enabled: true                    # Specifies whether to install MinIO Sample Tenant
minio_tenant_servers: 4                       # The number of MinIO Tenant nodes
minio_tenant_volumes_per_server: 4            # The number of volumes per servers
minio_deploy_test_mode: true                  # true (Test Mode) - use a file as loop device when creating storage
                                              # called "virtual block device" which is useful for test or automation purpose
                                              # false (Performance Mode) - use an actual NVME or SSD device when creating storage
{%- endif %}
